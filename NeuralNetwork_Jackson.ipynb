{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# San Francisco Crime prediction \n",
    "# Based on 2 layer neural net and count featurizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.feature_extraction import DictVectorizer as DV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from matplotlib.colors import LogNorm\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from copy import deepcopy\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline,FeatureUnion\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainDF=pd.read_csv(\"data/train.csv\")\n",
    "#trainDF=trainDF.sample(15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up wrong X and Y values (very few of them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = trainDF['Category']\n",
    "sss = StratifiedShuffleSplit(labels, train_size=0.5,random_state=0)\n",
    "for train_index, dev_index in sss:\n",
    "    train_data,dev_data=trainDF.iloc[train_index],trainDF.iloc[dev_index]\n",
    "    train_labels,dev_labels=labels[train_index],labels[dev_index]\n",
    "dev_data.index=range(len(dev_data))\n",
    "train_data.index=range(len(train_data))\n",
    "train_labels.index=range(len(train_labels))\n",
    "dev_labels.index=range(len(dev_labels))\n",
    "labels.index=range(len(labels))\n",
    "trainDF.index=range(len(trainDF))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_time(x):\n",
    "    DD=datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\")\n",
    "    time=DD.hour#*60+DD.minute\n",
    "    day=DD.day\n",
    "    month=DD.month\n",
    "    year=DD.year\n",
    "    return time,day,month,year\n",
    "\n",
    "def get_season(x):\n",
    "    summer=0\n",
    "    fall=0\n",
    "    winter=0\n",
    "    spring=0\n",
    "    if (x in [5, 6, 7]):\n",
    "        summer=1\n",
    "    if (x in [8, 9, 10]):\n",
    "        fall=1\n",
    "    if (x in [11, 0, 1]):\n",
    "        winter=1\n",
    "    if (x in [2, 3, 4]):\n",
    "        spring=1\n",
    "    return summer, fall, winter, spring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RemoveColumnsTransformer(TransformerMixin):\n",
    "    \n",
    "    def __init__(self,cols=[],include=True):\n",
    "        self.cols = cols\n",
    "        self.include = include\n",
    "\n",
    "    def transform(self, X,y=None, **transform_params):\n",
    "        print(\"Dropping \" , self.cols)\n",
    "        if self.include:\n",
    "            x_num_train = X.loc[:,self.cols]\n",
    "            \n",
    "        else:\n",
    "            x_num_train = X.drop( self.cols, axis = 1,errors='ignore') \n",
    "        return X\n",
    "      \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "class ScalerTransform(TransformerMixin):\n",
    "    \n",
    "    def __init__(self,cols=[],include=True):\n",
    "        self.cols = cols\n",
    "        self.include = include\n",
    "\n",
    "    def transform(self, X,y=None, **transform_params):\n",
    "        print(\"Scaler\")\n",
    "        if self.include:\n",
    "            x_num_train = X.loc[:,self.cols]\n",
    "        else:\n",
    "            x_num_train = X.drop( self.cols, axis = 1 )\n",
    "        return self.scaler.transform(x_num_train)\n",
    "      \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        if self.include:\n",
    "            x_num_train = X[self.cols]\n",
    "        else:\n",
    "            x_num_train = X.drop( self.cols, axis = 1 )      \n",
    "        self.scaler = preprocessing.StandardScaler().fit(x_num_train)\n",
    "        return self\n",
    "\n",
    "\n",
    "class DatesTransformer(TransformerMixin):\n",
    "\n",
    "    def __init__(self,datecol=\"Dates\"):\n",
    "        self.datecol = datecol\n",
    "\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        if(self.datecol in X):\n",
    "            print(\"Dates\")\n",
    "            DD =X[self.datecol].apply(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))\n",
    "            time,day,month,year  =  DD.dt.hour, DD.dt.day,DD.dt.month,DD.dt.year  \n",
    "            return np.array([time,day,month,year]).T\n",
    "        return \n",
    "        \n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "class SeasonsTransformer(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        print(\"Seasons\")\n",
    "\n",
    "        def get_season(x):\n",
    "            summer=0\n",
    "            fall=0\n",
    "            winter=0\n",
    "            spring=0\n",
    "            if (x in [5, 6, 7]):\n",
    "                summer=1\n",
    "            if (x in [8, 9, 10]):\n",
    "                fall=1\n",
    "            if (x in [11, 0, 1]):\n",
    "                winter=1\n",
    "            if (x in [2, 3, 4]):\n",
    "                spring=1\n",
    "            return summer, fall, winter, spring\n",
    "        \n",
    "        if('Dates' in X):\n",
    "            DD =X.loc[:,'Dates'].apply(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))\n",
    "            awake=DD.dt.hour.apply(lambda x: 1 if (x==0 or (x>=8 and x<=23)) else 0)\n",
    "            summer, fall, winter, spring=zip(*DD.dt.month.apply(get_season))\n",
    "            return  np.array([awake,summer,fall,winter,spring]).T     \n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "class AddIsIntersectionTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        print(\"Intersection\")\n",
    "\n",
    "        if('Address' in X):\n",
    "            return (X.loc[:,\"Address\"].apply(lambda x: 1 if \"/\" in x else 0))[:,None]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self    \n",
    "    \n",
    "class StreetNamesTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        def getstreet(x):\n",
    "            if  \"/\" in x: \n",
    "                return pd.Series(list(re.findall(r'([A-Z0-9 ]+)\\s/\\s([A-Z0-9 ]+)', x)[0]))\n",
    "            else: \n",
    "                street = re.findall(r'\\d+.*?\\s+Block of ([A-Z0-9 ]+)', x)[0]\n",
    "                return pd.Series([street, street])\n",
    "        \n",
    "        print(\"StreetNames\")\n",
    "        streets = X[\"Address\"].apply(getstreet)\n",
    "        streets.columns = [\"street1\",\"street2\"]\n",
    "        return pd.concat([X2, streets], axis=1)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self    \n",
    "    \n",
    "class LogOddsTransformer(TransformerMixin):\n",
    "    def __init__(self,togroup):\n",
    "        self.togroup = togroup\n",
    "        \n",
    "\n",
    "        \n",
    "    def transform(self, X, **transform_params):\n",
    "        print \"LogOdds transform\"\n",
    "        col = X[self.togroup] \n",
    "        address_features=col.apply(lambda x: self.logodds.setdefault(tuple(x) if len(x) > 1 else x[0],self.default_logodds),axis=1)\n",
    "        PA = col.apply(lambda x: self.logoddsPA.setdefault(tuple(x) if len(x) > 1 else \"\",0),axis=1)\n",
    "        return np.hstack((address_features,PA[:,np.newaxis]))\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        print(\"LogOdds\")\n",
    "        categories=sorted(y.unique())\n",
    "        X2 = X.assign(Category=y.astype('object'))\n",
    "        C_counts=X2.groupby([\"Category\"]).size()\n",
    "        A_C_counts=X2.groupby(self.togroup +[\"Category\"]).size()\n",
    "        A_counts=X2.groupby(self.togroup).size()\n",
    "        addresses=A_counts.keys()\n",
    "        logodds={}\n",
    "        logoddsPA={}\n",
    "        MIN_CAT_COUNTS=2\n",
    "        default_logodds=np.log(C_counts/len(X2))-np.log(1.0-C_counts/float(len(X2)))\n",
    "        for addr in addresses:\n",
    "            PA=A_counts[addr]/float(len(X2))\n",
    "            logoddsPA[addr]=np.log(PA)-np.log(1.-PA)\n",
    "            logodds[addr]=deepcopy(default_logodds)\n",
    "            for cat in A_C_counts[addr].keys():\n",
    "                if (A_C_counts[addr][cat]>MIN_CAT_COUNTS) and A_C_counts[addr][cat]<A_counts[addr]:\n",
    "                    PA=A_C_counts[addr][cat]/float(A_counts[addr])\n",
    "                    logodds[addr][cat]=np.log(PA)-np.log(1.0-PA)\n",
    "            #logodds[addr]=pd.Series(logodds[addr])\n",
    "            #logodds[addr].index=range(len(categories))\n",
    "        self.logodds=logodds\n",
    "        self.default_logodds = default_logodds\n",
    "        self.logoddsPA=logoddsPA\n",
    "        return self       \n",
    "    \n",
    "class MarkDuplicatesTransformer(TransformerMixin):\n",
    "    def __init__(self,cols,include=True):\n",
    "        self.cols = cols\n",
    "        self.include = include\n",
    "\n",
    "    def transform(self, X, **transform_params):  \n",
    "        print(\"Duplicates\")\n",
    "        if self.include:\n",
    "            X2 = X[self.cols]\n",
    "        else:\n",
    "            X2 = X.drop( self.cols, axis = 1 ,errors='ignore')\n",
    "        return (pd.Series(X2.duplicated()|X2.duplicated(keep=\"last\")).apply(int))[:,None]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self        \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "class FeatureVecotrizer(TransformerMixin):\n",
    "    def __init__(self,cols,include=True):\n",
    "        self.cols = cols\n",
    "        self.include = include\n",
    "\n",
    "    def transform(self, X,y=None, **transform_params):\n",
    "        print(\"Vectorizer\")\n",
    "        if self.include:\n",
    "            x_cat_train = X[self.cols]\n",
    "            x_num_train = X.drop( self.cols, axis = 1 )\n",
    "        else:\n",
    "            x_num_train = X[self.cols]\n",
    "            x_cat_train = X.drop( self.cols, axis = 1 )\n",
    "        \n",
    "        x_cat_train.fillna( 'NA', inplace = True )\n",
    "        vec_x_cat_train  =pd.get_dummies(x_cat_train)\n",
    "        return vec_x_cat_train\n",
    "\n",
    "    def fit(self, X,y=None, **fit_params):\n",
    "        x_cat_train = X[self.cols]\n",
    "        x_cat_train.fillna( 'NA', inplace = True )\n",
    "        x_cat_train = x_cat_train.T.to_dict().values()\n",
    "        return  self\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PapaDocNeuralNetworkModel(Sequential):\n",
    "    \n",
    "    def __init__(self,hn=32,dp=0.5,layers=1,epochs=1,batches=64,verbose=0,*args,**kwargs):\n",
    "        self.hn = hn\n",
    "        self.dp = dp\n",
    "        self.Nlayers = layers\n",
    "        self.epochs=epochs\n",
    "        self.batches = batches\n",
    "        self.verbose = verbose\n",
    "        super(PapaDocNeuralNetworkModel,self).__init__(*args,**kwargs)\n",
    "    \n",
    "    def fit(self,X_train,y_train,X_test=None,y_test=None,*args,**kwargs):\n",
    "        y_train=y_train.astype('category')\n",
    "        input_dim=X_train.shape[1]\n",
    "        output_dim=len(y_train.unique())\n",
    "        Y_train=y_train.cat.rename_categories(range(len(y_train.unique())))\n",
    "        self.add(Dense(input_dim=input_dim, output_dim=self.hn, init='glorot_uniform'))\n",
    "        self.add(PReLU(input_shape=(self.hn,)))\n",
    "        self.add(Dropout(self.dp))\n",
    "        for i in range(self.Nlayers):\n",
    "            self.add(Dense(input_dim=self.hn, output_dim=self.hn, init='glorot_uniform'))\n",
    "            self.add(PReLU(input_shape=(self.hn,)))\n",
    "            self.add(BatchNormalization())\n",
    "            self.add(Dropout(self.dp))\n",
    "\n",
    "        self.add(Dense(input_dim=self.hn, output_dim=output_dim, init='glorot_uniform'))\n",
    "        self.add(Activation('softmax'))\n",
    "        self.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "        if X_test is not None:\n",
    "            X_test=X_test.as_matrix()\n",
    "            y_test=y_test.astype('category')\n",
    "            Y_test=np_utils.to_categorical(y_test.cat.rename_categories(range(len(y_test.unique()))))\n",
    "            super(PapaDocNeuralNetworkModel,self).fit(X_train, Y_train, nb_epoch=self.epochs, \\\n",
    "                            batch_size=self.batches,verbose=self.verbose,\\\n",
    "                              validation_data=(X_test,Y_test),*args,**kwargs)\n",
    "        else:\n",
    "            super(PapaDocNeuralNetworkModel,self).fit(X_train, Y_train, nb_epoch=self.epochs, \\\n",
    "                                                      batch_size=self.batches,verbose=self.verbose,*args,**kwargs)\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class EnsembleClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Ensemble classifier for scikit-learn estimators.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    clf : `iterable`\n",
    "      A list of scikit-learn classifier objects.\n",
    "    weights : `list` (default: `None`)\n",
    "      If `None`, the majority rule voting will be applied to the predicted class labels.\n",
    "        If a list of weights (`float` or `int`) is provided, the averaged raw probabilities (via `predict_proba`)\n",
    "        will be used to determine the most confident class label.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, clfs, weights=None):\n",
    "        self.clfs = clfs\n",
    "        self.weights = weights\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        print(\"Ensemble\")\n",
    "        \"\"\"\n",
    "        Fit the scikit-learn estimators.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        X : numpy array, shape = [n_samples, n_features]\n",
    "            Training data\n",
    "        y : list or numpy array, shape = [n_samples]\n",
    "            Class labels\n",
    "\n",
    "        \"\"\"\n",
    "        for clf in self.clfs:\n",
    "            clf.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        X : numpy array, shape = [n_samples, n_features]\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "\n",
    "        maj : list or numpy array, shape = [n_samples]\n",
    "            Predicted class labels by majority rule\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.classes_ = np.asarray([clf.predict(X) for clf in self.clfs])\n",
    "        if self.weights:\n",
    "            avg = self.predict_proba(X)\n",
    "\n",
    "            maj = np.apply_along_axis(lambda x: max(enumerate(x), key=operator.itemgetter(1))[0], axis=1, arr=avg)\n",
    "\n",
    "        else:\n",
    "            maj = np.asarray([np.argmax(np.bincount(self.classes_[:,c])) for c in range(self.classes_.shape[1])])\n",
    "\n",
    "        return maj\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        X : numpy array, shape = [n_samples, n_features]\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "\n",
    "        avg : list or numpy array, shape = [n_samples, n_probabilities]\n",
    "            Weighted average probability for each class per sample.\n",
    "\n",
    "        \"\"\"\n",
    "        self.probas_ = [clf.predict_proba(X) for clf in self.clfs]\n",
    "        avg = np.average(self.probas_, axis=0, weights=self.weights)\n",
    "\n",
    "        return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_EPOCHS=20\n",
    "N_HN=128\n",
    "N_LAYERS=1\n",
    "DP=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "papadocFeatures =  Pipeline([(\"Features\",FeatureUnion([\n",
    "                    (\"XY\",ScalerTransform([\"X\",\"Y\"])), \\\n",
    "                    (\"DateToTime\",DatesTransformer()), \\\n",
    "                    (\"Intersection\",AddIsIntersectionTransformer()),\\\n",
    "                    (\"Vectorizer\",FeatureVecotrizer(['PdDistrict','DayOfWeek'])),\\\n",
    "                    #(\"LogsOdds\",Pipeline([\\\n",
    "                                #(\"Street\",StreetNamesTransformer()),\\\n",
    "                                #(\"LogOdds\",LogOddsTransformer([\"PdDistrict\",\"street1\",\"street2\"]))\\\n",
    "                    #])),\\\n",
    "                    (\"LogOdds\",LogOddsTransformer([\"Address\"])),\\\n",
    "\n",
    "                    (\"MarkDuplicates\",MarkDuplicatesTransformer([\"PdDistrict\",\"DayOfWeek\",\"Dates\",\"Address\"])),\\\n",
    "                    (\"Seasons\",SeasonsTransformer())\n",
    "                    ],n_jobs=3)),                   \n",
    "                    (\"ScaleEverything\",preprocessing.StandardScaler())])\n",
    "\n",
    "\n",
    "\n",
    "ensemblepipe = Pipeline([(\"Features\",papadocFeatures),\\\n",
    "               ('eclf', EnsembleClassifier([LogisticRegression(C=.01),\n",
    "                    PapaDocNeuralNetworkModel(hn=N_HN,layers=N_LAYERS,epochs=N_EPOCHS,verbose=2,dp=DP)\n",
    "]))])\n",
    "\n",
    "model = ensemblepipe.fit(train_data,train_labels.astype('category'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"train\", log_loss(train_labels.astype('category'), model.predict_proba(train_data))\n",
    "print \"test\", log_loss(dev_labels.astype('category'), model.predict_proba(dev_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testDF=pd.read_csv(\"data/test.csv\")\n",
    "testDF.index=range(len(testDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = ensemblepipe.fit(trainDF,labels.astype('category'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predDF=pd.DataFrame(model.predict_proba(testDF),columns=sorted(labels.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predDF.to_csv(\"crimeSF_NN_logodds.csv\",index_label=\"Id\",na_rep=\"0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
