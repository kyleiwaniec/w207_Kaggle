{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Creating training data...\n",
      "Creating training labels...\n",
      "Input dimensions: 18\n",
      "------------------------------------------------------------\n",
      "('Fold', 0)\n",
      "------------------------------------------------------------\n",
      "Building model...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got multiple values for keyword argument 'init'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e271f4a79cdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Building model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-e271f4a79cdc>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(input_dim, output_dim, hn, dp, layers)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'glorot_uniform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got multiple values for keyword argument 'init'"
     ]
    }
   ],
   "source": [
    "#### THIS IS FROM https://www.kaggle.com/smerity/sf-crime/fighting-crime-with-keras/run/12741 ####\n",
    "\n",
    "import csv\n",
    "import gzip\n",
    "###\n",
    "import numpy as np\n",
    "###\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def get_data(fn):\n",
    "    data = []\n",
    "    with open(fn) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        data = [row for row in reader]\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_fields(data, fields):\n",
    "    extracted = []\n",
    "    for row in data:\n",
    "        extract = []\n",
    "        for field, f in sorted(fields.items()):\n",
    "            info = f(row[field])\n",
    "            if type(info) == list:\n",
    "                extract.extend(info)\n",
    "            else:\n",
    "                extract.append(info)\n",
    "        extracted.append(np.array(extract, dtype=np.float32))\n",
    "    return extracted\n",
    "\n",
    "\n",
    "def shuffle(X, y, seed=1337):\n",
    "    np.random.seed(seed)\n",
    "    shuffle = np.arange(len(y))\n",
    "    np.random.shuffle(shuffle)\n",
    "    X = X[shuffle]\n",
    "    y = y[shuffle]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def preprocess_data(X, scaler=None):\n",
    "    if not scaler:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    return X, scaler\n",
    "\n",
    "\n",
    "def dating(x):\n",
    "    date, time = x.split(' ')\n",
    "    y, m, d = map(int, date.split('-'))\n",
    "    time = time.split(':')[:2]\n",
    "    time = int(time[0]) * 60 + int(time[1])\n",
    "    return [y, m, d, time]\n",
    "\n",
    "days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "districts = ['BAYVIEW', 'CENTRAL', 'INGLESIDE', 'MISSION', 'NORTHERN', 'PARK', 'RICHMOND', 'SOUTHERN', 'TARAVAL', 'TENDERLOIN']\n",
    "labels = 'ARSON,ASSAULT,BAD CHECKS,BRIBERY,BURGLARY,DISORDERLY CONDUCT,DRIVING UNDER THE INFLUENCE,DRUG/NARCOTIC,DRUNKENNESS,EMBEZZLEMENT,EXTORTION,FAMILY OFFENSES,FORGERY/COUNTERFEITING,FRAUD,GAMBLING,KIDNAPPING,LARCENY/THEFT,LIQUOR LAWS,LOITERING,MISSING PERSON,NON-CRIMINAL,OTHER OFFENSES,PORNOGRAPHY/OBSCENE MAT,PROSTITUTION,RECOVERED VEHICLE,ROBBERY,RUNAWAY,SECONDARY CODES,SEX OFFENSES FORCIBLE,SEX OFFENSES NON FORCIBLE,STOLEN PROPERTY,SUICIDE,SUSPICIOUS OCC,TREA,TRESPASS,VANDALISM,VEHICLE THEFT,WARRANTS,WEAPON LAWS'.split(',')\n",
    "data_fields = {\n",
    "    'X': lambda x: float(x),\n",
    "    'Y': lambda x: float(x),\n",
    "    'DayOfWeek': lambda x: days.index(x) / float(len(days)),\n",
    "    'Address': lambda x: [1 if 'block' in x.lower() else 0],\n",
    "    'PdDistrict': lambda x: [1 if x == d else 0 for d in districts],\n",
    "    'Dates': dating,  # Parse 2015-05-13 23:53:00\n",
    "}\n",
    "label_fields = {'Category': lambda x: labels.index(x.replace(',', ''))}\n",
    "\n",
    "print('Loading training data...')\n",
    "raw_train = get_data('/Users/koza/Documents/UCBerkeley/207/project/w207_Kaggle/data/train.csv')\n",
    "print('Creating training data...')\n",
    "X = np.array(get_fields(raw_train, data_fields), dtype=np.float32)\n",
    "print('Creating training labels...')\n",
    "y = np.array(get_fields(raw_train, label_fields))\n",
    "del raw_train\n",
    "\n",
    "X, y = shuffle(X, y)\n",
    "X, scaler = preprocess_data(X)\n",
    "Y = np_utils.to_categorical(y)\n",
    "\n",
    "input_dim = X.shape[1]\n",
    "output_dim = len(labels)\n",
    "print('Input dimensions: {}'.format(input_dim))\n",
    "\n",
    "\n",
    "def build_model(input_dim, output_dim, hn=32, dp=0.5, layers=1):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_dim, hn, init='glorot_uniform'))\n",
    "    model.add(PReLU((hn,)))\n",
    "    model.add(Dropout(dp))\n",
    "\n",
    "    for i in range(layers):\n",
    "        model.add(Dense(hn, hn, init='glorot_uniform'))\n",
    "        model.add(PReLU((hn,)))\n",
    "        model.add(BatchNormalization((hn,)))\n",
    "        model.add(Dropout(dp))\n",
    "\n",
    "    model.add(Dense(hn, output_dim, init='glorot_uniform'))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "    \n",
    "def log_loss(y_true,y_pred):\n",
    "    eps = 1e-15\n",
    "    y_pred = y_pred/y_pred.sum(axis=1)[:,np.newaxis]\n",
    "    y_pred = np.maximum(eps,y_pred)\n",
    "    y_pred = np.minimum(1-eps,y_pred)\n",
    "    y_pred = np.log(y_pred)\n",
    "    ll = 0\n",
    "    for i in range(len(y_true)):\n",
    "        ll -= y_pred[i,int(y_true[i])]\n",
    "    return ll/len(y_true)\n",
    "\n",
    "EPOCHS = 1\n",
    "BATCHES = 128\n",
    "HN = 64\n",
    "RUN_FOLDS = True\n",
    "nb_folds = 5\n",
    "kfolds = KFold(len(y), nb_folds)\n",
    "av_ll = 0.\n",
    "f = 0\n",
    "if RUN_FOLDS:\n",
    "    for train, valid in kfolds:\n",
    "        print('---' * 20)\n",
    "        print('Fold', f)\n",
    "        print('---' * 20)\n",
    "        f += 1\n",
    "        X_train = X[train]\n",
    "        X_valid = X[valid]\n",
    "        Y_train = Y[train]\n",
    "        Y_valid = Y[valid]\n",
    "        y_valid = y[valid]\n",
    "\n",
    "        print(\"Building model...\")\n",
    "        model = build_model(input_dim, output_dim, HN)\n",
    "\n",
    "        print(\"Training model...\")\n",
    "\n",
    "        model.fit(X_train, Y_train, nb_epoch=EPOCHS, batch_size=BATCHES, validation_data=(X_valid, Y_valid), verbose=0)\n",
    "        valid_preds = model.predict_proba(X_valid)\n",
    "        ll = log_loss(y_valid, valid_preds)\n",
    "        print(\"LL:\", ll)\n",
    "        av_ll += ll\n",
    "    print('Average LL:', av_ll / nb_folds)\n",
    "\n",
    "print(\"Generating submission...\")\n",
    "\n",
    "model = build_model(input_dim, output_dim, HN)\n",
    "model.fit(X, Y, nb_epoch=EPOCHS, batch_size=BATCHES, verbose=0)\n",
    "\n",
    "print('Loading testing data...')\n",
    "raw_test = get_data('/Users/koza/Documents/UCBerkeley/207/project/w207_Kaggle/data/test.csv')\n",
    "print('Creating testing data...')\n",
    "X_test = np.array(get_fields(raw_test, data_fields), dtype=np.float32)\n",
    "del raw_test\n",
    "X_test, _ = preprocess_data(X_test, scaler)\n",
    "\n",
    "print('Predicting over testing data...')\n",
    "preds = model.predict_proba(X_test, verbose=0)\n",
    "\n",
    "with gzip.open('sf-nn.csv.gz', 'wt') as outf:\n",
    "    fo = csv.writer(outf, lineterminator='\\n')\n",
    "    fo.writerow(['Id'] + labels)\n",
    "\n",
    "    for i, pred in enumerate(preds):\n",
    "        fo.writerow([i] + list(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
